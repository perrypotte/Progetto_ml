{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class CSVImageDataset(data.Dataset):\n",
    "    def __init__(self, data_root, csv, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.data = pd.read_csv(csv)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        im_path, im_label = self.data.iloc[i]['Path'], self.data.iloc[i].Label\n",
    "        # il dataset contiene alcune immagini in scala di grigi\n",
    "        # convertiamo tutto in RGB per avere delle immagini consistenti\n",
    "        im = Image.open(join(self.data_root, im_path)).convert('RGB')\n",
    "        \n",
    "        #Per fare eventualmente data augmentation o quelle come toTensor?\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "        return im, im_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessing_trasnform = transforms.Compose([\n",
    "    transforms.Resize((resolution,resolution)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "dataset_train=CSVImageDataset('euro_dataset/train/images','train.csv',transform=preprocessing_trasnform)\n",
    "dataset_valid=CSVImageDataset('euro_dataset/valid/images','valid.csv',transform=preprocessing_trasnform)\n",
    "dataset_test=CSVImageDataset('euro_dataset/test/images','test.csv',transform=preprocessing_trasnform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Coins: 1\n",
      "torch.Size([3, 640, 640])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4322)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# im,lab=dataset_train[55]\n",
    "im,lab=dataset_train[150]\n",
    "print('#Coins:',lab)\n",
    "print(im.shape)\n",
    "im.mean()\n",
    "im[0].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import squeezenet1_0\n",
    "from torchvision.models import SqueezeNet1_0_Weights\n",
    "from torch import nn\n",
    "\n",
    "model= squeezenet1_0(weights=SqueezeNet1_0_Weights.DEFAULT)\n",
    "\n",
    "num_class=1\n",
    "model.classifier[1]=nn.Conv2d(512,num_class,kernel_size=(1,1),stride=(1,1))\n",
    "model.num_classes=num_class\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEDIA E VARIANZA DA CALCOLARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_mean=0\n",
    "green_mean=0\n",
    "blue_mean=0\n",
    "for image in dataset_train:\n",
    "    red_mean+=image[0][0].sum()# somma di tutti i pixel\n",
    "    green_mean+=image[0][1].sum()# somma di tutti i pixel\n",
    "    blue_mean+=image[0][2].sum()# somma di tutti i pixel\n",
    "\n",
    "#dividiamo per il numero di immagini molt numero di pixel\n",
    "red_mean=red_mean/(len(dataset_train)*(resolution)**2)\n",
    "green_mean=green_mean/(len(dataset_train)*(resolution**2))\n",
    "blue_mean=blue_mean/(len(dataset_train)*(resolution**2))\n",
    "\n",
    "#deviazione standard\n",
    "red_std=0\n",
    "green_std=0\n",
    "blue_std=0\n",
    "for image in dataset_train:\n",
    "    red_std+=((image[0][0]-red_mean)**2).sum()\n",
    "    green_std+=((image[0][1]-green_mean)**2).sum()\n",
    "    blue_std+=((image[0][2]-blue_mean)**2).sum()\n",
    "\n",
    "#sqrt della varianza\n",
    "red_std=np.sqrt(red_std/(len(dataset_test)*(resolution)**2))\n",
    "green_std=np.sqrt(green_std/(len(dataset_test)*(resolution**2)))\n",
    "blue_std=np.sqrt(blue_std/(len(dataset_test)*(resolution**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6418) tensor(0.6095)\n",
      "tensor(0.6030) tensor(0.6272)\n",
      "tensor(0.5569) tensor(0.6531)\n"
     ]
    }
   ],
   "source": [
    "print(red_mean,red_std)\n",
    "print(green_mean,green_std)\n",
    "print(blue_mean,blue_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([red_mean, green_mean, blue_mean], [red_std, green_std, blue_std])\n",
    "])\n",
    "\n",
    "from torchvision import transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),##?????\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([red_mean, green_mean, blue_mean], [red_std, green_std, blue_std])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset_train=CSVImageDataset('euro_dataset/train/images','train.csv',transform=train_transform)\n",
    "dataset_valid=CSVImageDataset('euro_dataset/valid/images','valid.csv',transform=test_transform)\n",
    "dataset_test=CSVImageDataset('euro_dataset/test/images','test.csv',transform=test_transform)\n",
    "\n",
    "caltech101_train_loader=(DataLoader(dataset_train,batch_size=32,num_workers=0,shuffle=True))\n",
    "caltech101_valid_loader=(DataLoader(dataset_valid,batch_size=32,num_workers=0))\n",
    "caltech101_test_loader=(DataLoader(dataset_test,batch_size=32,num_workers=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "\n",
    "class AverageValueMeter():\n",
    "    def init(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "\n",
    "    def add(self, value, num):\n",
    "        self.sum += value*num\n",
    "        self.num += num\n",
    "\n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.sum/self.num\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainval(model, train_loader, test_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr, momentum=momentum)\n",
    "    # meters\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    # writer\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    # device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    # definiamo un dizionario contenente i loader di training e test\n",
    "    loader = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader\n",
    "    }\n",
    "    # inizializziamo il global step\n",
    "    global_step = 0\n",
    "    os.makedirs(\"weights\", exist_ok=True)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1} of {epochs}\")\n",
    "        # iteriamo tra due modalità: train e test\n",
    "        for mode in ['train']:  #, 'test'\n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(mode=='train'): # abilitiamo i gradienti solo in training\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    x = batch[0].to(device) # portiamoli sul device corretto\n",
    "                    y = batch[1].to(device)\n",
    "\n",
    "                    y=y.float()\n",
    "\n",
    "                    output = model(x)\n",
    "                    print(y.dtype)\n",
    "                    print(output.dtype)\n",
    "                    # aggiorniamo il global step\n",
    "                    # conterrà il numero di campioni visti durante il training\n",
    "                    n = x.shape[0] # numero di elementi nel batch\n",
    "                    global_step += n\n",
    "                    print(output.view(-1).shape,y.shape)\n",
    "                    l = criterion(output.view(-1), y)\n",
    "                    print(l)\n",
    "                    if mode=='train':\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    acc = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
    "                    loss_meter.add(l.item(), n)\n",
    "                    acc_meter.add(acc, n)\n",
    "                    \n",
    "                    # Loggiamo i risultati iterazione per iterazione solo durante il training\n",
    "                    if mode=='train':\n",
    "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
    "            \n",
    "            # una volta finita l'epoca (sia nel caso di training che test, loggiamo le stime finali)\n",
    "            writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
    "            \n",
    "            # conserviamo i pesi del modello alla fine di un ciclo di training e test\n",
    "            torch.save(model.state_dict(), '%s-%d.pth'%(exp_name, e+1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 5\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.Size([32]) torch.Size([32])\n",
      "tensor(20.8369, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m squeezenet_caltech101_finetuned\u001b[38;5;241m=\u001b[39m\u001b[43mtrainval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcaltech101_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcaltech101_valid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msqueezenet_caltech101_finetuning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[81], line 46\u001b[0m, in \u001b[0;36mtrainval\u001b[1;34m(model, train_loader, test_loader, exp_name, lr, epochs, momentum, logdir)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\perri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\perri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "squeezenet_caltech101_finetuned=trainval(model,caltech101_train_loader,caltech101_valid_loader,exp_name='squeezenet_caltech101_finetuning',lr=0.001,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
